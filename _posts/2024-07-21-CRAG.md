---
title: "[논문 리뷰] CRAG - Comprehensive RAG Benchmark"
date: 2024-07-21 21:48:28 -0400
categories: NLP
tags:
  - CRAG
  - LLM
  - RAG

use_math: false
toc: true
---

# [논문 리뷰] CRAG - Comprehensive RAG Benchmark

## 요약

- RAG 시스템 성능 평가를 위해 4409개의 QA / Contents 로 이루어진 CRAG 벤치마크 개발
- CRAG을 통해 RAG 시스템의 성능 및 개선방향 파악 가능(최신/빈도수낮음/복잡한 질문 등)
- LLM에 바로 연결된 RAG 시스템은 CRAG 기준 정확도가 매우 낮은 편(≤44%)이나 SOTA 시스템 등은 어느 정도 성능(≤63%)을 보여주고 있음

## 데이터셋

- 4409개의 QA + 각 Q마다 50개의 reference HTML 페이지

    - 5가지 Domain : Finance, Sports, Music, Movie, Open
    - 8가지 질문 카테고리


    ![table2](https://github.com/lih0905/lih0905.github.io/blob/master/_posts/images/240721/table2.png?raw=true)

    - 빈도에 따라 Head, torso, tail
    - 시기에 따라 real-time, fast-changing, slow-changing, static

- mock Knowledge Graph API (260만개의 entities)
- Constructing QA from KGs
    - (Simple static and dynamic questions) entity type / meaningful relation (e, r)을 템플릿에 넣어서 Question 생성, 원래의 value를 Answer로 생성
    - (multi-hop questions) (e1, r1, e2), (e1, r2, e3) 를 조합하여 e3이 Answer, 나머지로 Question 생성
- Constructing QA from web contents
    1. Annotator들이 웹 검색 할만한 질문 생성
    2. 그 질문에 대한 검색 결과 생성(최대 50개)
    3. 검색 결과에 질문의 답이 있으면 그것을 Answer로 설정, 없으면 Annotator가 직접 탐색해서 적절한 웹페이지를 찾아서 Answer로 설정

## Tasks

각 QA셋 하나마다 다음의 3가지 Task를 정의

1. Retrieval Summarization : 각 질문마다 5개의 웹페이지 제공(these are link, but not guaranteed, to be relevant to the question)
2. KG and Web Retrieval Augmentation : question을 parsing한 값을 입력으로 받아서 structured data를 리턴하는 KG API를 content에 추가
3. End-to-end RAG : 2번에 추가로 각 question마다 50개의 웹페이지를 content로 사용

## Metrics

- 생성된 정답은 perfect / acceptable / missing / incorrect 중 하나로 분류
- Score_h는 각 경우마다 1 / 0.5 / 0 / -1 을 배점하며, human evalution
- model evaluation은 perfect / acceptable 을 모아서 accurate로 분류, 그외는 동일하며, 이 경우 Score_a 를 각각 1 / 0 / -1 로 배점
    - 생성한 Answer 가 ground truth 와 동일하면 accurate, 아니면 LLM(GPT-3.5-turbo, Llama-3-70-instruct)을 이용해서 3중 하나로 분류

## Benchmarking

### straightforward RAG solutions 평가 결과

![table5](https://github.com/lih0905/lih0905.github.io/blob/master/_posts/images/240721/table5.png?raw=true)

- LLM만 사용했을 땐 단지 33.5%의 정확도로 매우 낮은 편이며 RAG 사용 시 43.6%까지 개선
- 그러나 RAG 사용 시 Hallucination도 증가 → retrieval noise를 피해 좋은 결과를 탐색해야함
- Task1 → Task2 시 accuracy/score 모두 증가 → KG의 정확도가 도움이 되나 증가폭이 크지 않음
- real-time / fast-changing QA를 잘 맞추지 못하며, set answer, post-processing, false-premises 유형도 어려워함

### SOTA RAG system

![table6](https://github.com/lih0905/lih0905.github.io/blob/master/_posts/images/240721/table6.png?raw=true)

- SOTA solution들은 걍 만든 RAG에 비해 훨씬 성능이 좋음
- 코파일럿과 제미나이가 나머지 두개에 비해 Halluciation이 적은 편
- set과 false-premise의 성능이 크게 향상됨