---
title: "GloVe 모델이란?"
date: 2019-08-26 23:30:28 -0400
categories: NLP
tags:
  - paper review
  - GloVe
use_math: true
toc : true
---
'19.8.26 edited.

## 개요 

이번 글에서는 단어의 벡터화 모델 중 GloVe에 대해 이해해보려고 한다. GloVe 모델은 Jeffrey Pennington, Richard Socher, 그리고 Christopher D. Manning(CS224n 강의로 유명!)이 저술한 [Glove: Global Vectors for Word Representation (2014)](https://nlp.stanford.edu/pubs/glove.pdf) 논문에서 소개되었다.

단어의 벡터 모델은 크게 SVD(singular value decomposition)를 이용한 행렬 분해 방식과 문맥에 같이 등장하는 단어들을 이용한 방법으로 나뉜다. 전자의 대표적인 방법은 LSA(latent sementic analysis)이며, 후자로는 저번 [포스트](https://lih0905.github.io/nlp/Word2vec/)에서 소개한 Word2Vec(skip-gram) 모델 등이 있다. 그러나 두 방법은 모두 단점이 있는데, 행렬 분해 방식은 단어의 전역적 출연 빈도에 따른 통계를 활용하지만 단어간 유사성 파악에는 적합하지 않으며, skip-gram 모델은 전역적 빈도가 아닌 근방의 단어만을 이용하여 훈련된다.

저자들은 이 논문에서 단어간의 전역적 동시 등장 빈도를 토대로 훈련되는 동시에 벡터 공간의 선형성을 보존하는 모델로서 GloVe를 제안하였으며, GloVe는 단어 유사성 테스트에서 state-of-the-art 성능인 75%를 기록하였다고 한다.

> 단어의 유사성 테스트는 [ king : queen = man : ?? ] 라는 문제의 답을 단어에 대응되는 벡터들의 코사인 유사도를 통해 찾는 테스트이며, 자세한 내용은 이전 skip-gram 모델의 소개 [포스트](https://lih0905.github.io/nlp/Word2vec/)에 서술하였다. 


## GloVe 모델의 아이디어

말뭉치에서 단어의 빈도는 단어 표현의 학습에서 있어 가장 중요한 원천이라고 할 수 있다. 그러나 여전히 빈도로부터 `의미`가 어떻게 생성되는 지에 대해서는 여전히 많은 의문점이 남아 있다. GloVe 모델은 이런 전역적 빈도를 어떻게 활용할 지에 대한 고민에서 탄생한 모델이며, 모델의 이름부터가 Global Vectors(전역 벡터)를 줄인 것이라고 한다.

논문에서 사용되는 몇 가지 표기법을 정의하자. 본문에는 명확히 언급되어 있지 않지만, 먼저 한 단어로부터 얼마까지 떨어져있는 단어를 하나의 문맥(context)으로 인정할 것인지를 결정하는 `Window size`가 고정되어 있다고 하자. 이제, 단어 $i$의 문맥에 다른 단어 $j$가 등장하는 총 횟수를 $X_{ij}$라고 하고, 단어간 동시 발생 횟수를 나타내는 행렬 $X$를 다음과 같이 정의하자.

$$
X := \left( X_{ij} \right)
$$

또한 $X_i = \sum_k X_{ik}$를 단어 $i$의 문맥에 등장하는 모든 단어의 총 등장 횟수라고 하자. 마지막으로, 단어 $i$의 문맥에 $j$가 등장할 확률 $P_{ij}$를 다음과 같이 정의하자.

$$
P_{ij} := P(j\vert i) =  \frac{X_{ij}}{X_i}
$$

GloVe 모델의 핵심 아이디어는, 두 단어 $i$와 $j$의 관계는 단순 빈도가 아니라, 또 다른 임의의 단어 $k$가 두 단어와 연관되어 있는 지를 분석함으로써 더욱 깊게 파악할 수 있다는 것이다. 이를  구체적인 예를 통해 이해해보자. 

단어 $i$는 ice, $j$는 steam이라고 가정하자. 이 두 단어의 관계를 다양한 $k$에 대해 $P_{ik}/P_{jk}$를 계산함으로서 파악하고자 한다. 단어 $k$가 solid일 경우, 이 단어는 ice와는 유의미한 관계가 있으나 steam과는 그렇지 않을 것으로 예상할 수 있다. 이런 경우 $P_{ik}/P_{jk}$는 상대적으로 큰 값을 가질 것이다. $k$가 gas인 경우는 그 반대에 해당할 것이며, 단어 water와 fashion은 각각 두 단어와 모두 연관이 있거나 혹은 없는 경우에 해당한다. 실제 큰 말뭉치를 통해 계산한 각각의 확률은 다음과 같다.

|확률|$k$=solid|$k$=gas|$k$=water|$k$=fashion|
|--|:--:|:--:|:--:|:--:|
|$P(k\vert \text{ice})$| $1.9 \times 10^{-4}$|$6.6\times 10^{-5}$|$3.0\times 10^{-3}$|$1.7 \times 10^{-5}$|
|$P(k\vert \text{steam})$| $2.2 \times 10^{-5}$|$7.8\times 10^{-4}$|$2.2\times 10^{-3}$|$1.8 \times 10^{-5}$|
|$P(k\vert \text{ice})/P(k\vert \text{steam})$| $8.9$|$8.5\times 10^{-2}$|$1.36$|$0.96$|

따라서 빈도 사이의 비율을 이용하면 두 단어 중 하나와만 관련있는 단어들(solid, gas)을 두 단어 모두와 관련 있거나 없는 단어들(water, fashion)과 쉽게 분리할 수 있다. 또한 하나와만 관련 있는 단어들 또한 어떤 단어와 관련 있는지 또한 파악할 수 있다.

## GloVe 모델의 구조

앞서 설명한 아이디어를 통해, 단어 표현 학습은 동시 발생 빈도 자체보다는 동시 발생 빈도 사이의 비율을 이용해야하는 것이 좋을 것이라는 기대를 가지게 되었다. 이를 구체적으로 구현해보자.

(continued...)


## 참고 자료

1. [Stanford, CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/)

2. [GloVe, word representation](https://lovit.github.io/nlp/representation/2018/09/05/glove/)

3. [GloVe를 이해해보자!](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/09/glove/)