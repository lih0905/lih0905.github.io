---
title: "2019년 읽은 논문들"
date: 2019-08-20 23:35:28 -0400
categories: 잡설
tags:
  - Papers
  - 논문
  - review

use_math: true
#toc: true
---

2019년 읽은 논문 정리.

1. [Recent Trends in Deep Learning Based Natural Language Processing, Young et al. (2017)](https://arxiv.org/pdf/1708.02709.pdf)  
    - 날짜 : 2019-07-23
    - 카테고리 : NLP
    - 내용 : 2017년 기준으로 자연어처리의 여러 Task 및 연구 방법에 대한 리뷰 논문. 현 시점에서는 Attention, Transformers 계열에 대한 소개가 없어 아쉬운 부분이 있지만, 본격적으로 딥러닝이 도입되기 전 자연어처리 연구 방향이 어떠했는지 배경 지식을 쌓기에 좋을 것으로 보인다.
    <br>

1. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling,  Chung et al. (2014)](https://arxiv.org/pdf/1412.3555.pdf)
    - 날짜 : 2019-08-13
    - 카테고리 : NLP
    - 내용 : 바닐라 RNN 구조를 개선한 대표적인 모델은 LSTM이 있었으나, 2014년 조경현 교수님 등이 GRU 모델을 제안한다. 이 논문에서는 LSTM과 GRU 모델의 구조를 비교하고, 여러 시계열 데이터에서 두 모델의 성능을 비교한다. 두 모델의 성능은 Task 별로 우위가 달라져, 결국 본인의 Task마다 비교해봐야 하는 것 같다.
    <br>

1. [Efficient Estimation of Word Representations in Vector Space, Mikolov et al. (2013)](https://arxiv.org/pdf/1301.3781.pdf)
    - 날짜 : 2019-08-15
    - 카테고리 : NLP
    - 내용 : 자연어처리의 시작은 텍스트를 컴퓨터가 학습할 수 있는 대상으로 변환하는 것이다. 이 논문은 단어 벡터화의 새로운 지평을 연 Word2Vec 모델을 소개한다. 이 모델의 핵심 아이디어는 특정 단어가 주어졌을 때 그 주변 단어들의 등장 확률을 증가시키는 방향으로 학습시키는 것이다. 잘 학습된 Word2Vec 모델은 단어들 사이의 의미적, 형태적 관계를 담아낼 수 있다.
    - [논문요약글](https://lih0905.github.io/nlp/Word2vec/)
    <br>

1. [Distributed Representations of Words and Phrases and their Compositionality, Mikolov et al. (2013)](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
    - 날짜 : 2019-08-20
    - 카테고리 : NLP
    - 내용 : 앞서 소개한 Word2Vec은 성능은 뛰어나지만, 손실함수를 계산할 때마다 소프트맥스 연산을 수행하기 때문에 계산량이 굉장히 많다는 단점이 있다. 이 논문에서는 모델의 계산량을 줄이기 위한 방법으로 계층적 소프트맥스, 네거티브 샘플링, 빈도에 따른 서브샘플링을 소개하고 있으며, 또한 New York Times, Korean Air 등 두 단어 이상의 길이를 가지는 구절을 토큰화하는 방법에 대해서도 논의한다.
    - [논문요약글](https://lih0905.github.io/nlp/Word2vec_2/)
    <br>

1. [GloVe: Global Vectors for Word Representation, Pennington et al. (2014)](https://nlp.stanford.edu/pubs/glove.pdf)
    - 날짜 : 2019-08-27
    - 카테고리 : NLP
    - 내용 : 이 논문에서 단어간의 전역적 동시 등장 빈도를 토대로 훈련되는 동시에 벡터 공간의 선형성을 보존하는 모델로서 GloVe를 제안하고 있다. GloVe 모델의 핵심 아이디어는, 두 단어 사이의 관계는 단순 둘의 빈도가 아니라 또 다른 임의의 단어와 각각이 얼마나 연관되어 있는지를 분석함으로써 더욱 깊게 파악할 수 있다는 것이다. 
    - [논문요약글](https://lih0905.github.io/nlp/GloVe/)
    <br>

1. [Enriching Word Vectors with Subword Information, Bojanowski et al. (2016)]()
    - 날짜 : 2019-08-30
    - 카테고리 : NLP
    - 내용 : Facebook AI 랩에서 만든 오픈소스 라이브러리 fastText는 단어 표현 및 텍스트 분류 등의 기능이 있으며, 무엇보다 굉장히 빠르고 가벼운 것으로 유명한데, 이 논문이 라이브러리의 이론적 배경을 제공하였다. fastText는 Word2Vec 모델을 ‘부분 단어’라는 개념을 도입하여 개선한다. 부분 단어는 단어 `where`를 다음과 같은 부분어들로 분해하는 것이다.
     `{<wh, whe, her, ere, re>, <whe,wher,here,ere>, <wher,where,here>,where}`
     이를 통해 단어의 형태적 특성 학습 및 기존 단어장에 등장하지 않는(Out of vocabulary) 단어의 의미 또한 파악할 수 있게 된다.
    - [논문요약글](https://lih0905.github.io/nlp/fasttext1/)    
    <br>

1. [Bag of Tricks for Efficient Text Classification, Joulin et al. (2016)](https://arxiv.org/pdf/1607.01759.pdf)
    - 날짜 : 2019-09-03
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, Wu et al. (2016)](https://arxiv.org/pdf/1609.08144.pdf)
    - 날짜 : 2019-09-10
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Sequence to Sequence Learning with Neural Networks, Sutskever et al. (2014)](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)
    - 날짜 : 2019-09-17
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau et al. (2014)](https://arxiv.org/pdf/1409.0473.pdf)
    - 날짜 : 2019-09-24
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Attention Is All You Need, Vaswani et al. (2017)](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
    - 날짜 : 2019-10-01
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Convolutional Neural Networks for Sentence Classification, Kim (2014)](https://arxiv.org/pdf/1408.5882.pdf)
    - 날짜 : 2019-10-08
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Deep contextualized word representations, Peters et al. (2018)](https://arxiv.org/pdf/1802.05365.pdf)
    - 날짜 : 2019-11-30
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [, et al. ()]()
    - 날짜 : 2019-
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [, et al. ()]()
    - 날짜 : 2019-
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [, et al. ()]()
    - 날짜 : 2019-
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [, et al. ()]()
    - 날짜 : 2019-
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [, et al. ()]()
    - 날짜 : 2019-
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [, et al. ()]()
    - 날짜 : 2019-
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [, et al. ()]()
    - 날짜 : 2019-
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [, et al. ()]()
    - 날짜 : 2019-
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [, et al. ()]()
    - 날짜 : 2019-
    - 카테고리 : NLP
    - 내용 : 
    <br>

