---
title: "Word2Vec 논문 리뷰"
date: 2019-08-21 01:48:28 -0400
categories: "눈문_리뷰"
tags:
  - word2vec
  - skip-gram
  - negative_sampling
  - hierarchical_softmax
use_math: true
---
'19.8.21 edited.

### 개요

현재 진행중인 [NLP 스터디](https://github.com/ai-robotics-kr/nlp_study)에서는 매주 돌아가며 논문을 한 편씩 공부하고 있다. 스터디를 준비하며 정리한 내용을 요약하여 올려보고자 한다. 이번주에 공부한 논문은 다음과 같다.

### [Distributed Representations of Words and Phrases and Their Compositionality], Mikolov et al (2014)



이 논문은 단어 벡터화의 새로운 지평을 연 Word2Vec 모델 및 그 개선법에 대해 공부한다. Word2Vec은 T. Mikolov 및 그 외 연구진에 의해 쓰인 `Efficient Estimation of Word Representations in vector space (2013)` 논문에서 처음 소개되었으며, 본 논문에서는 Word2Vec 모델을 개선하는 방안에 대해 주로 논의한다. Word2Vec 모델은 CBOW(Continuous Bag-of-Words)와 Skip-gram 방식으로 나뉘며, 일반적으로 Skip-gram 방식이 더 성능이 좋기에 널리 사용된다. 따라서 본 글에서는 Skip-gram 모델에 한정하여 논의하고자 한다.

단어를 벡터화하는 가장 간단한 방법은 원-핫-벡터를 이용하여 각 단어를 단위 벡터($\mathbf{e}_w$)로 표기하는 것이다. 그러나 이 방식은 단어의 갯수만큼 벡터의 차원이 커지게 되며, 동시에 벡터의 대부분이 $0$ 으로 채워져 메모리를 효율적으로 사용하지 못하는 단점이 있다. 또한 각 단어 벡터간 내적이 $0$ 이므로 단어 사이의 관계에 대한 정보를 전혀 얻지 못하게 된다. 따라서 단어를 밀도 높은 벡터(dense vector)로 나타내는 모델을 개발하고자 하는 연구가 계속 되어 왔다. 

<center> 
<img src = 'https://miro.medium.com/max/1838/1*YvOdGp73pOHmYGHKqkx5wQ.png'>

[source : https://miro.medium.com/max/1838/1*YvOdGp73pOHmYGHKqkx5wQ.png]
</center>

Skip-gram은 단어들을 정해진 차원의 벡터 공간에 임베딩하는 모델이다(일반적으로 벡터 공간의 차원 << 단어의 갯수). 먼저 임의의 값으로 벡터들을 초기화한 후, 특정 단어가 주어졌을 때 그 주변 단어들의 등장 확률을 증가시키는 방향으로 학습하는 알고리즘이다. 가령 `I love him but he hates me.` 라는 문장을 생각해보자. 여기서 `him`이라는 단어를 기준으로 앞 뒤 두 단어들인 `I`,`love`,`but`,`he`의 발생 확률을 증가시키는 방향으로 학습하게 된다. 

잘 학습된 Skip-gram 모델은 단어에 대응되는 벡터들의 연산이 실제 단어 간의 의미 및 형태를 보존하게 되어, 텍스트 모델링에 큰 도움을 줄 수 있다. 다만 모델을 훈련하는 과정에서 계산량이 어마어마하기에 이를 개선하기 위해 계층적 소프트맥스, 네거티브 샘플링 등의 기법이 도입된다.


### Skip-gram 모델의 구조
주어진 훈련 텍스트에 등장하는 단어들의 집합을 Vocab이라고 하고, 이 집합의 크기를 $K$ 라 하자. 이 모델의 hyperparameter로 다음 두 가지를 결정해야 한다. 중심 단어를 기준으로 몇 번째 단어까지 문맥(context)으로 고려할 지 결정해야 하며, 이를 `WINDOW SIZE`라고 한다. 또한 단어들을 임베딩할 벡터 공간의 차원 $D$ 를 결정해야 한다. 

먼저 크기가 ($K$, $D$) 인 행렬 $V, U$를 임의값으로 초기화하자. 단어 집합의 $i$번째 단어 $w_i$에 대응되는 입력 벡터(input vector)를 $v_{i} = V_{i} \in \mathbb{R}^D$ , 출력 벡터(output vector)를 $u_{i} = U_i \in \mathbb{R}^D$라고 정의한다(행렬 $A$의 $i$번째 행을 $A_i$로 표기). 

임의의 중심 단어 $c$에 대해($c$번째 단어), 점수 벡터(score vector) $z = U \cdot v_c $ 를 계산한 후 이 벡터에 소프트맥스를 취하면 다음 확률 벡터 $\hat{y}\in \mathbf{R}^K$ 를 얻을 수 있다.

 $$\hat{y} = \text{softmax}(z) =\left( \frac{\exp(u_j^t v_c)}{\sum_{i=1}^{K}\exp(u_i^t v_c)}\right)_{j \in \{ 1,\ldots,K \}}$$

따라서 중심 단어 $c$ 에 대해 단어 $o$ 가 문맥에 발생할 확률 $p(o\vert c)$ 을 다음과 같이 정의한다.

$$
p(o|c) := \hat{y}_o = \frac{\exp(u_o^t v_c)}{\sum_{i=1}^{K}\exp(u_i^t v_c)}
$$

주어진 텍스트가 $\{ w_1, w_2, \ldots, w_T\}$라고 토큰화되어 있을 때, 우리의 목표는 다음 손실함수를 최소화하는 것이다($m$은 WINDOW SIZE).

$$
J :=- \frac{1}{T} \sum_{t=1}^{T} \sum_{j=0, j\ne m}^{2m} \log p(w_{t+j}|w_t)
$$

이는 다시 말해, 텍스트에서 주어진 중심 단어마다 문맥에 등장하는 단어의 확률을 최대한 증가시키는 방향으로 학습을 진행한다는 의미이다(논문에서는 위에 적힌 함수에 $-1$ 이 곱해진 형태로 주어져있지만, 크로스-엔트로피 함수와의 일치성을 위해 해당 형태를 고려). 중심 단어 $w_c$ 하나를 고정한 경우의 손실함수는 다음과 같다. 

$$
\begin{array}{lcl}
 J &=& - \log p(w_{c-m}, \ldots, w_{c-1},w_{c+1},\ldots, w_{c+m})\\
 & =& - \log \prod_{j=0, j\ne m}^{2m} p(w_{c-m+j}|w_c) \\
 & =& - \sum_{j=0, j\ne m}^{2m} \log \frac{\exp(u_{c-m+j}^t v_c)}{\sum_{i=1}^{K}\exp(u_i^t v_c)} \\
 &=&-\sum_{j=0, j\ne m}^{2m} u_{c-m+j}^t v_c + 2m \log\sum_{i=1}^{K}\exp(u_i^t v_c) 
 \end{array}
$$

참고로 이는 원-핫-벡터 $y_{c-m+j}$ 들에 대한 $\hat{y}$ 의 크로스-엔트로피 함수

$$\sum_{j=0, j\ne m}^{2m} H(\hat{y}, y_{c-m+j})$$

와 같다. 위 손실함수의 정의로부터 계산을 통해 다음을 확인할 수 있다.

$$
\begin{array}{lcl}
\frac{\partial{J}}{\partial{v_c}} &=& - \sum_j {u_j} + 2m \sum_{w=1}^K \frac{\exp(u_w^t v_c)}{\sum_{i=1}^K \exp(u_i^tv_c)}u_w \\
&=&  - \sum_j {u_j} + 2m \sum_{i=1}^K p(w|c) u_w,\\
\\
\frac{\partial{J}}{\partial{u_o}} &=& -v_c + p(o|c) v_c \quad (o \text{ is a context word}),\\
\\
\frac{\partial{J}}{\partial{u_w}} &=& p(w|c) v_c \qquad \quad (w \text{ is not a context word}) .
\end{array}
$$ 

따라서 그래디언트 업데이트를 다음 식을 통해 수행할 수 있다($\alpha$ 는 learning rate).

$$ V = V - \alpha \frac{\partial{J}}{\partial{V}},\quad U = U - \alpha \frac{\partial{J}}{\partial{Q}} $$

이제 중심 벡터에 대해 해당 학습을 반복하며 단어 $w_i$ 의 두 가지 벡터 임베딩 $v_i = V_i $, $u_i = U_i$ 를 얻을 수 있다. 이 중 어떤 것을 벡터 표현으로 선택해도 성능에 크게 차이는 없다고 알려져 있으며, 일반적으로 입력 벡터 $v_i$ 를 사용한다.


### Skip-gram모델의 장단점
잘 학습된 Word2Vec 모델의 경우 각 단어에 대응되는 벡터는 단어들 사이의 형태적, 의미적 관계를 모두 보존하게 된다. 예를 들면 다음 대응들을 내포하고 있다.

* king : man = queen : ?? 
	* 이 경우는 단어의 의미(권위/성별)에 대한 추론으로 woman을 답한다.
* do : did = play : ?? 
	* 이 경우는 단어의 형태(현재형:과거형)에 대한 추론으로 played를 답한다. 


위 대응 관계는 각 단어의 벡터 임베딩을 통한 다음 계산을 통해 확인할 수 있다. 

$$
v_{\text{king}} - v_{\text{man}} + v_{\text{queen}} \quad \simeq \quad v_{\text{woman}}
$$

여기서 기호 $\simeq $ 은 단어들에 대응되는 벡터 공간에서 좌변의 벡터와 가장 가까운(cosine similarity가 가장 높은) 벡터가 우변의 벡터라는 의미이다.

따라서 Word2Vec 모델을 사용하면 단어들을 훨씬 작은 공간에 임베딩하여 메모리를 효율화하는 것 뿐 아니라, 단어들 사이의 의미적, 형태적 관계까지 모델링할 수 있다는 장점이 있다. 

그러나 이런 vanilla skip-gram 모델의 경우 치명적인 단점이 있다. 실제 언어를 모델링하는 경우 단어의 갯수 $K$ 가  $10^5$ ~ $10^7$ 정도로 굉장히 큰 편인데, 그래디언트를 계산할 때마다 <b>소프트맥스 연산에 드는 계산량이 $K$에 비례</b>하기 때문에 굉장히 비효율적이다. 따라서 다음 기법들을 도입하여 계산량을 줄이고자 한다.

### Hierarchical Softmax
계층적 소프트맥스(hierarchical softmax)는 기존 소프트맥스를 대신하여 가능한 출력별 확률을 계산하는 모델이다. 이 모델은 이진 트리를 사용하여 모든 단어를 표현한 후 해당 단어에 도달할 확률을 구해나간다. 트리의 각 잎(leaf, 트리의 말단)은 단어에 해당하며, 뿌리(root)로부터 각 잎까지는 유일한 경로(path)로 연결된다. 이 모델은 단어의 입력 벡터($v_i$)만 존재하며, 대신 트리의 각 마디(node)에 대응되는 벡터가 존재하여 이를 훈련시키도록 한다. 각 마디에서는 왼쪽과 오른쪽 중 어떤 자식을 선택할 지에 대한 확률이 주어진다. 

<center>
<img src ='https://shuuki4.files.wordpress.com/2016/01/hsexample.png'>

[source : https://shuuki4.files.wordpress.com/2016/01/hsexample.png]
</center>

$L(w)$ 를 뿌리로부터 단어 $w$ 까지 도달하는 경로의 길이라고 하고, $n(w,i)$ 를 뿌리부터 단어 $w$ 사이의 경로(유일함!) 중 $i$ 번째 마디라고 정의하자. 따라서 $n(w,1)$ 은 뿌리에 해당하고, $n(w,L(w))$ 는 단어 $w$ 자신이다. 내부 마디 $n$ 마다 자식 하나를 임의로 고정하고 이를 $ch(n)$ 이라 표기하자. 이제 중심 단어 $w_i$ 의 문맥에 단어 $w$ 가 등장할 확률은 다음과 같이 정의한다:

$$
p(w|w_i) =\prod_{j=1}^{L(w)-1} \sigma([n(w,j+1) = ch(n(w,j))]\cdot {u_{n(w,j)}}^t v_{w_i}).
$$ 

여기서 $[x]$는 $x$가 참일 때 $1$, 거짓일 때 $-1$로 주어지는 함수이며 $\sigma$는 시그모이드 함수이다.  그리고 $v_{w_i}$는 단어 $w_i$의 입력 벡터, $u_{n(w,j)}$는 내부 마디 $n(w,j)$ 의 벡터 표현이다.

해당 식이 도출되는 과정을 좀 더 살펴보자. 뿌리부터 단어 $w$ 사이의 내부 마디 $n$ 가 주어져 있을 때, 해당 마디의 고정된 자식 $ch(n)$ 은 왼쪽 자식이라고 가정해보자. 왼쪽 자식으로 진행할 확률 $p(n, \text{left})$ 을 다음과 같이 정의하자.

$$p(n, \text{left}) = \sigma({u_n}^t v_{w_i})  $$

그러면 해당 값은 위에서 정의된 

$$(\star) \qquad \sigma([n(w,j+1) = ch(n(w,j))]\cdot {u_{n(w,j)}}^t v_{w_i})$$

와 일치한다. 
마찬가지로 $n$ 에서 오른쪽 자식으로 진행할 확률 $p(n, \text{right})$ 을 생각하면

$$ p(n, \text{right}) = 1 -p(n, \text{left}) = 1-  \sigma({u_n}^t v_{w_i}) $$

이다. 시그모이드 함수는 임의의 $x$에 대해 $\sigma(x) + \sigma(-x) = 1$ 를 만족하므로, 

$$p(n, \text{right}) =\sigma(-{u_{n}}^t v_{w_i}) $$

이고, 역시 위의 등식 ($\star$)이 성립한다. 따라서 위에서 정의된 $p(w\vert w_i)$ 는 뿌리로부터 단어 $w$ 에 도달할 확률임을 알 수 있다. 다시 말해 $p(w\vert w_i)$ 는 뿌리로부터 단어에 도달하는 랜덤 워크임을 알 수 있으며, 뿌리에서 출발하면 결국 어떤 단어에 도달하게 되므로 자연스레 다음 식을 얻을 수 있다.

$$
\sum_{w=1}^{K} p(w|w_i) = 1
$$ 

따라서 $p(w\vert w_i)$ 는 확률분포를 이루게 된다! 기존 Skip-gram 모델에서 계산량이 증가함에도 불구하고 소프트맥스 함수를 적용하는 이유 또한 각 단어의 예측 확률의 합이 $1$ 이 되어 확률 분포를 이루고, 이로 인해 추정이 가능해지기 때문이다.

계층적 소프트맥스 모델의 손실 함수는 $-\log(p(w\vert w_i))$ 로 주어지며, 해당 함수의 그래디언트는 $L(w)$ 개의 항을 계산하면 구할 수 있다. 다만 여기서는 출력 벡터를 업데이트하는 것이 아니라 트리의 내부 마디들의 벡터를 업데이트하게 된다. 이렇게 정의된 모델은 계산량이 

$$\text{Average}_{w}(L(w)) \sim \log_2 (K)$$

에 비례하므로 기존 소프트맥스 모델에 비해 획기적으로 줄어들게 된다. 

<center>
<img src ='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQz70OZgcO1KP3ppOv9LxQvv_57zIez8zdz9QsT5XRzjPEEk14K9Q'>

[source : https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQz70OZgcO1KP3ppOv9LxQvv_57zIez8zdz9QsT5XRzjPEEk14K9Q]
</center>

본 논문에서는 Huffman 트리를 이용하여 계층적 소프트맥스를 구현하였다. Huffman 트리는 가장 빈도가 낮은 단어부터 두개씩 묶어가면서 전체 단어를 연결하여 생성한다. 따라서 가장 빈도가 높은 단어가 가장 마지막에 묶이게 되므로 트리의 뿌리로부터 가깝게 생성된다.


### Negative Sampling
네거티브 샘플링은 계층형 소프트맥스 대신 계산량을 줄이기 위해 사용할 수 있는 방법이다. 매 훈련 스텝마다 모든 단어에 대해 연산을 수행하는 대신 일부 부정적 예제를 샘플링하여 사용하는 방법이다. 단어 $w$, $c$를 고려하자. 이제 $D$라는 문장에서  $(w,c)$가 $c$가 중심 단어, $w$가 그 문맥에 발생할 확률을 $p(D=1\vert w,c)$, 발생하지 않을 확율을 $p(D=0 \vert w,c)$이라고 하자. 

먼저 시그모이드 함수를 이용하여 $p(D=1\vert w,c)$ 를 다음과 같이 정의하자:

$$
p(D=1|w,c) = \sigma(u_w^t v_c) = \frac{1}{1+\exp(-u_w^t v_c)}
$$ 

그러면 모델링의 목표는 다음과 같다.

$$
\begin{array}{ll}
&\text{maximize} \log\left(\prod_{(w,c)\in D} p(D=1|w,c) \prod_{(w,c)\in \tilde{D}} p(D=0|w,c) \right) \\
= &\text{maximize} \log\left(\prod_{(w,c)\in D} p(D=1|w,c) \prod_{(w,c)\in \tilde{D}} (1-p(D=1|w,c)) \right) \\
= &\text{maximize} \left(\sum_{(w,c)\in D} \log p(D=1|w,c) +\sum_{(w,c)\in \tilde{D}} \log(1-p(D=1|w,c)) \right) \\
= &\text{maximize} \left(\sum_{(w,c)\in D} \log (\frac{1}{1+\exp(-u_w^t v_c)}) +\sum_{(w,c)\in \tilde{D}} \log(\frac{1}{1+\exp(u_w^t v_c)}) \right)
\end{array}
$$ 

여기서  $\tilde{D}$는 말이 되지 않는 문장 등을 샘플링한 집합이다. 예를 들어 `stock boil fish is toy`와 같은 문장의 발생 확률은 매우 낮게 계산되어야 하기 때문에, 위 수식에서 해당 집합에 포함된 단어들은 발생 확률이 낮아지도록 설정하였다.

Skip-gram 모델에서 새로운 손실 함수는 다음과 같이 정의된다.

$$
-\log \sigma(u^t_{c-m+j}\cdot v_c) - \sum_{k=1}^{|\tilde{D}|} \log \sigma(-\tilde{u}^t_k \cdot v_c)
$$ 

위 식에서 $\{\tilde{u}_k\}$들은 샘플링된 부정적 예제들이다. 이때, 각 단어들은 다음 확률 분포를 사용하여 샘플링하는 것이 효과적이라고 알려져있다.

$$
p(w_i) \sim \frac{f(w_i)^{3/4}}{\sum_j f(w_j)^{3/4}}
$$ 

여기서 $f(w_j)$는 훈련 데이터 중 단어 $w_i$의 발생 빈도이다. 위와 같이 $3/4$ 승을 한 분포를 사용하는 이유는 드물게 발생하는 단어의 샘플링 비율을 높이기 위함이다.



### Subsampling on frequency
매우 큰 텍스트 데이터의 경우  `in`, `the`, `a` 등의 단어는 너무 자주 등장하지만 실제로는 별 의미를 담고 있지 않다. 예를 들어 `France`와 `Paris`가 같이 발생한 경우 모델의 훈련에 도움이 되지만 `France`와 `the`가 같이 발생한 경우는 큰 의미를 가지기 어렵다. 따라서 이런 단어의 발생 빈도에 따른 불균형을 해소하기 위해 서브샘플링 기법을 도입한다.

훈련 데이터로 주어진 단어 $w_i$의 발생 빈도가 $f(w_i)$인 경우, 훈련 데이터에서 해당 단어는 다음 확률로 제거하도록 한다:

$$
p(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}
$$ 

여기서 $t$는 해당 확률을 적용하는 한계점으로, 보통 $10^{-5}$ 정도를 사용한다. 다시 말해, 발생 빈도가 $t$ 이하인 단어는 제거하지 않는다는 의미이다.

이런 공식을 적용하면 발생 빈도가 $t$ 이상인 단어는 크게 제거하면서도 발생 빈도의 순서 자체는 보존하는 장점이 있다. 실제로 이렇게 서브샘플링을 적용할 경우 훈련 속도뿐 아니라 정확성까지도 크게 향상되었다.
