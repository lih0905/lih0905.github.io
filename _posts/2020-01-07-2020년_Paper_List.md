---
title: "2020년 읽은 논문들"
date: 2020-01-07 23:15:28 -0400
categories: 잡설
tags:
  - Papers
  - 논문
  - review

use_math: true
#toc: true
---

2020년 읽은 논문 정리.

1. [Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks, Xiao et al. (2017)](https://www.comp.nus.edu.sg/~xiangnan/papers/ijcai17-afm.pdf) ([코드](https://github.com/hexiangnan/attentional_factorization_machine)) 
    - 날짜 : 2020-01-06
    - 카테고리 : 추천시스템
    - 내용 : 추천 시스템의 feature를 모두 동등하게 평가하여 분해하는 모델 FM(Factorization Machines)에 추가로 attention을 적용하여 각 feature별 가중치까지 학습하게 만든 AFM(Attentional Factorization Machines) 소개
    - [논문요약글](https://lih0905.github.io/%EC%B6%94%EC%B2%9C/AFM/)
    <br>

1. [Deep Autoencoder for Recommender Systems: Parameter Influence Analysis, Hoan et al. (2018)](https://arxiv.org/abs/1901.00415) ([코드](https://github.com/heroddaji/flexEncoder))
    - 날짜 : 2020-01-07
    - 카테고리 : 추천시스템
    - 내용 : DAE(Deep Autoencoder)를 이용하여 각 아이템에 대한 유저들의 평점을 입력 벡터로 하여 MLP 및 dropout 적용 후 자기 자신을 다시 생성하게 하는 오토인코더를 이용하여 해당 아이템을 추천하지 않은 유저들의 평점을 예측하는 추천 시스템 모델. 특히 다양한 hyperparameter를 반영할 수 있도록 실험 환경을 구축하여 최적의 parameter 탐색에 초점을 둠. 
    <br>

1. [HoAFM: A High-order Attentive Factorization Machine for CTR Prediction, Tao et al. (2019)](http://staff.ustc.edu.cn/~hexn/papers/IPM19-HoAFM.pdf) ([코드](https://github.com/zltao/HoAFM/blob/master/HoAFM_v1.0.py))
    - 날짜 : 2020-01-08
    - 카테고리 : 추천시스템
    - 내용 : 1. 에서 소개한 AFM은 벡터간의 point-wise곱에 attention scalar를 곱하도록 설계된 모델이다. HoAFM은 이를 좀 더 일반화하여 attention scalar 대신 vector를 곱하는, 다시 말해 point-wise곱의 각 component별로 영향력을 다르게 반영하는 모델이다.
    <br>

1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al. (2018)](https://arxiv.org/abs/1810.04805) ([코드](https://github.com/dhlee347/pytorchic-bert/))
    - 날짜 : 2020-01-13
    - 카테고리 : NLP
    - 내용 : 그 유명한 BERT. Sentence를 토큰 단위로 Bidirectional Transformer layer에 통과시켜 pre-train시킨 후, 각 task에 대응하는 output 형태로 fine-tuning시키는 모델이다. Pre-train 과정에서 input token의 15%를 실제 학습에 사용하는데, 이중 80%는 마스킹, 10%는 랜덤한 다른 token, 10%는 입력 token을 그대로 사용하여 언어의 구조 및 단어 추론 등을 학습하도록 한다.
    <br>

1. [BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer, Sun et al. (2019)](https://arxiv.org/abs/1904.06690) ([코드](https://github.com/FeiSun/BERT4Rec))
    - 날짜 : 2020-01-14
    - 카테고리 : 추천시스템
    - 내용 : 제목부터 알 수 있듯이, BERT와 유사한 구조로 사용자가 클릭한 순차적 아이템을 일부 masking한 input을 Bidirectional Transformer를 이용하여 훈련시키는 모델이다. 다만 pre-train을 시키는 것은 아니며, 단지 bidirectional transformer를 활용한다는 점이 BERT와 유사하다.
    <br>

1. [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, Lan et al. (2019)](https://arxiv.org/abs/1909.11942) ([코드](https://github.com/google-research/ALBERT))
    - 날짜 : 2020-01-23
    - 카테고리 : NLP
    - 내용 : BERT의 경량화 방법에 대해 고민한 모델 중 하나인 ALBERT를 다룬 논문이다. 기존 BERT 모델에 추가로 Factorized embedding parametrization(임베딩/히든 스페이스의 디멘전을 다르게 설정), Cross-layer parameter sharing(모든 레이어의 파라미터를 공유), Sentence-order prediction(NSP 대신 문장의 순서에 대해 학습) 등의 요소를 도입하였다. 이를 통해 파라미터 수/학습 시간은 줄어들었으나 오히려 성능은 향상되는 놀라운 결과를 낳았다.
    <br>

1. [Session-based Recommendation with Graph Neural Networks, Wu et al. (2019)](https://arxiv.org/abs/1811.00855) ([코드](https://github.com/CRIPAC-DIG/SR-GNN))
    - 날짜 : 2020-01-30
    - 카테고리 : 추천시스템
    - 내용 : 세션을 그래프로 나타낸 후, 그래프의 Connecting Matrix를 이용하여 GRU structure를 통해 아이템의 벡터 표현을 학습한다. 이후 세션의 각 아이템에 대한 Attention value를 계산하여 세션의 벡터 표현을 구한 후, 이를 토대로 세션의 다음 아이템을 예측한다. 즉, Graph + GRU + Attention 기반의 세션 예측 모델이다.
    - [발표자료](https://github.com/lih0905/lih0905.github.io/raw/master/_posts/SR-GNN.pdf)
    <br>
	
1. [Are Sixteen Heads Really Better than One?, Michel et al. (2019)](https://arxiv.org/pdf/1905.10650) 
    - 날짜 : 2020-02-07
    - 카테고리 : NLP
    - 내용 : Transformer 계열 모델들은 Multi-heads를 통한 Self-Attention 메커니즘을 사용하고 있다. Multi-heads의 도입 목적은 각각의 Head들이 각자의 표현을 익혀 모델의 다양성을 증가시키기 위함이다. 그러나 이 논문에서는 Head를 하나씩 지워가면서 실험해본 결과, Head가 줄어든다고 꼭 성능이 저하되는 것은 아니라는 것을 발견했다. 또한 모든 Head중 중요한 Head가 있어, 이외의 Head들을 제거하면 모델의 파라미터 수 및 inference time의 감소를 유도할 수 있음을 입증한다.
    <br>
	
1. [Online Deep Learning: Learning Deep Neural Networks on the Fly, Sahoo et al. (2018)](https://www.ijcai.org/Proceedings/2018/0369.pdf)
    - 날짜 : 2020-03-04
    - 카테고리 : Online Learning
    <br>
	
1. [Autonomous Deep Learning: Continual Learning Approach for Dynamic Environments, Ashfahani et al (2020)](https://arxiv.org/abs/1810.07348v4)
    - 날짜 : 2020-03-04
    - 카테고리 : Online Learning
    - [코드](https://github.com/SeptivianaSavitri/adl_python)
    <br>

1. [Online Learning to Rank for Sequential Music Recommendation, Pereira et al.(2019)](https://homepages.dcc.ufmg.br/~rodrygo/wp-content/papercite-data/pdf/pereira2019recsys.pdf)
    - 날짜 : 2020-03-26
    - 카테고리 : 추천시스템, Online Learning
    - 내용 : 실시간 음악 추천 시스템을 구현하는 방법에 대한 논문. 먼저 기존 유저의 선호도를 이용하여 추천 리스트를 만들어둔 후, 유저의 선택(끝까지 듣는지 혹은 중간에 스킵하는지)에 따라 리스트를 실시간으로 학습시켜가는 방법론을 다루고 있다.
    <br>

1. [Error-Driven Incremental Learning in Deep Convolutional Neural Network for Large-Scale Image Classiﬁcation, Xiao et al. (2014)](https://dl.acm.org/doi/pdf/10.1145/2647868.2654926)
    - 날짜 : 2020-04-02
    - 카테고리 : Incremental Learning
    <br>

1. [iCaRL: Incremental Classifier and Representation Learning, Rebuffi et al. (2016)](http://openaccess.thecvf.com/content_cvpr_2017/papers/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.pdf)
    - 날짜 : 2020-04-09
    - 카테고리 : Incremental Learning
    - 내용 : 이미지 분류 태스크에서 클래스가 지속적으로 증가하는 경우에 어떤 식으로 모델을 구성해야 하는지를 다룬 논문. 먼저 기존 데이터를 토대로 훈련시킨 CNN모델을 생각한 후, 각 클래스별 대표 이미지(exemplar sets)를 클래스 전체 이미지의 모델 출력의 평균값과 가장 가까운 이미지들로 선택한다. 이제 새로운 데이터가 들어오면, 기존 클래스에 대해서는 distillation loss, 새로운 클래스에 대해서는 classification loss를 적용하여 모델을 훈련시킨다. 또한 모든 클래스에 대해서 지속해서 대표 이미지를 업데이트하여 모델을 유지시켜 나가는 모델이다.
    - [발표자료](https://www.youtube.com/watch?v=HCKi41BHDAk)
    <br>
    
1. [Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation, Raganato et al. (2020)](https://arxiv.org/pdf/2002.10260)
    - 날짜 : 2020-04-14
    - 카테고리 : NLP
    - 내용 : Transformer 계열의 모델에서 주로 쓰이는 Multi-heads attention에 대한 의구심을 더하는 논문이다. 이 논문에서는 Transformer의 기본 구조를 유지한 채, 8개의 Multi-heads중 7개는 현재/이전/다음 위치 등을 참조하는 Static head로 변경하고 하나의 Head만 학습시키는 모델의 성능을 측정한다. 그 결과, 원 모델과 아주 큰 차이를 보이지는 않는다는 점을 발견하며, 이런 Static head의 사용이 low-resource한 기계번역에 유용하게 쓰일 것이라고 주장한다.
    <br>
    
1. [Improving Language Understanding by Generative Pre-Training, Radford et al. (2019)](https://www.google.com/url?q=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&sa=U&ved=2ahUKEwiVhrzvlePpAhWbBIgKHTPiCGMQFjAJegQIABAB&usg=AOvVaw1UMlQhOKGc0dmX7SbHNNHo)
    - 날짜 : 2020-05-24
    - 카테고리 : NLP
    - 내용 : 이른바 GPT. Unsupervised pre-training으로 트랜스포머 디코더를 이용하고, 이후 supervised fine-tuning을 수행하는 모델. Fine-tuning 과정에서는 pre-traning loss를 도움 함수로 추가해준다. 토크나이저는 BPE를 사용한다.
    <br>

1. [XLNet: Generalized Autoregressive Pretraining for Language Understanding, Yang et al. (2019)](https://arxiv.org/pdf/1906.08237)
    - 날짜 : 2020-05-31
    - 카테고리 : NLP
    - 내용 : 한방향의 정보만 이해하는 GPT, 양방향이지만 [mask] 토큰의 존재로 인해 정보를 유실하는 BERT의 단점을 극복하기 위해 나온 모델.
    <br>

1. [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Dai et al. (2019)](https://arxiv.org/pdf/1901.02860.pdf)
    - 날짜 : 2020-06-07
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension, Lewis et al. (2019)](https://arxiv.org/abs/1910.13461)
    - 날짜 : 2020-06-21
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders, Blevins et al. (2020)](https://blvns.github.io/papers/acl2020.pdf)
    - 날짜 : 2020-06-28
    - 카테고리 : NLP
    - 내용 : WSD
    <br>

1. [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108)
    - 날짜 : 2020-07-12
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [RoBERTa: A Robustly Optimized BERT Pretraining Approach, Liu et al. (2019)](https://arxiv.org/pdf/1907.11692)
    - 날짜 : 2020-07-19
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Generative Data Augmentation for Commonsense Reasoning, Yang et al. (2020)](https://www.aclweb.org/anthology/2020.findings-emnlp.90.pdf)
    - 날짜 : 2020-07-26
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [A Primer in BERTology: What we know about how BERT works, Rogers et al. (2020)](https://arxiv.org/pdf/2002.12327.pdf)
    - 날짜 : 2020-08-09
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping, Dodge et al. (2020)](https://arxiv.org/pdf/2002.06305)
    - 날짜 : 2020-08-14
    - 카테고리 : NLP
    - 내용 : 일반적으로 자연어처리 분야에서 랜덤 시드는 reproducibility를 위해 시작부터 고정하는 경향이 있다. 그러나 이 논문에서는 랜덤 시드 또한 하나의 하이퍼 파라미터로 여겨야 하며, 실제로 시드를 변경하는 것만으로 모델의 성능에 큰 편차가 발생한다는 것을 입증하였다. 실제로 랜덤 시드를 정하기 위해서 본문에서 "Start many, stop early, continue some" 정책을 추천하고 있다.
    <br>
	
1. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Raffel et al. (2019)](https://arxiv.org/pdf/1910.10683) ([코드](https://github.com/google-research/text-to-text-transfer-transformer))
    - 날짜 : 2020-08-23
    - 카테고리 : NLP
    - 내용 : T5
    <br>
	
1. [CodeBERT: A Pre-Trained Model for Programming and Natural Languages, Feng et al. (2020)](https://arxiv.org/pdf/2002.08155.pdf?fbclid=IwAR2hve-hWnFo7eV6IjCyfi6pIc9ABP4fn1Bf7aVcwhJrOmVovWmUW6sjapU) 
    - 날짜 : 2020-08-30
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization, Zhang et al. (2019)](https://arxiv.org/pdf/1912.08777)
    - 날짜 : 2020-09-06
    - 카테고리 : NLP
    - 내용 : 이 논문은 추상 요약에 적합한 사전 학습 시 목적 함수가 무엇인지를 다루고 있다. PEGASUS는 문단에서 "중요하다고 여길 수 있는 문장"들을 선정해 마스킹한 뒤 Transformer 인코더에 입력한다. 그리고 디코더는 해당 정보를 활용해 마스킹된 "Pseudo 중요 문장"들을 생성한다. 또한 마스킹이 되어 있지 않은 문단에 대해서는 여러가지 추출 방법을 통해 사전 학습을 진행한다. 그 결과 대부분의 벤치마크에서 SOTA를 기록하였다.
    <br>

1. [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, Clark et al. (2020)](https://openreview.net/pdf?id=r1xMH1BtvB) ([코드](https://github.com/google-research/electra))
    - 날짜 : 2020-09-13
    - 카테고리 : NLP
    - 내용 : 
    <br>

1. [Longformer: The Long-Document Transformer, Beltagy et al. (2020)](https://arxiv.org/pdf/2004.05150) ([코드](https://github.com/allenai/longformer))
    - 날짜 : 2020-09-27
    - 카테고리 : NLP
    - 내용 : 긴 길이의 텍스트를 임베딩하기 위한 모델을 구현한 논문이다. 일반적으로 Self-Attention은 텍스트의 길이의 제곱에 비례하는데, 이 논문에서는 이를 피하기 위해 대신 Sparse Self-Attention을 취한다. 이를 위해 Sliding window attention, global attention 등의 새로운 개념을 도입하였다.
    <br>

1. [Recipes for building an open-domain chatbot, Roller et al. (2020)](https://arxiv.org/pdf/2004.13637)
    - 날짜 : 2020-10-11
    - 카테고리 : NLP
    - 내용 : 페이스북에서 공개한 챗봇 모델 Blender를 다루는 논문이다. 이 모델은 성격, 공감, 지식이라는 3가지 특성을 조합하여 생성된 모델로서, 이를 통해 실제 사람과 유사한 대답을 하기도 하며 또한 위키피디아 검색을 수행하기 때문에 QA에도 뛰어나다.
    <br>
	
1. [Language Models are Few-Shot Learners, Brown et al. (2020)](https://arxiv.org/pdf/2005.14165)
    - 날짜 : 2020-10-25
    - 카테고리 : NLP
    - 내용 : GPT3...
    <br>
	
1. [Beyond Accuracy: Behavioral Testing of NLP models with CheckList, Ribeiro et al. (2020)](https://arxiv.org/pdf/2005.04118) ([코드](https://github.com/marcotcr/checklist))
    - 날짜 : 2020-11-08
    - 카테고리 : NLP
    - 내용 : 자연어처리 모델을 평가할 때 정확도는 좋은 메트릭이기는 하나, 단순히 정확도가 높다고 좋은 모델이라고 단언할 수는 없다. 따라서, 개별 테스트 케이스를 여럿 작성해서 이에 대한 모델의 답변을 통해 모델의 성능을 평가하는 방법도 사용되고 있는데, 이는 테스트 케이스 제작에 들어가는 노력 또한 무시할 수 없다는 단점이 있다. 이 논문은 이 두 Trade-off 사이에서 적절한 합의점을 찾을 수 있도록 체크리스트를 제시한다. 이를 통해 테스트 케이스를 작성한 경우, 실제로 모델의 성능을 유의미하게 평가할 수 있을 것으로 기대된다.
    <br>