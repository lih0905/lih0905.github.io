---
title: "2020년 읽은 논문들"
date: 2020-01-07 23:15:28 -0400
categories: 잡설
tags:
  - Papers
  - 논문
  - review

use_math: true
#toc: true
---

2020년 들어 읽은 논문 정리.

1. [[Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks], Xiao et al. (2017)](https://www.comp.nus.edu.sg/~xiangnan/papers/ijcai17-afm.pdf) ([코드](https://github.com/hexiangnan/attentional_factorization_machine)) 
    - 날짜 : 2020-01-06
    - 카테고리 : 추천시스템
    - 내용 : 추천 시스템의 feature를 모두 동등하게 평가하여 분해하는 모델 FM(Factorization Machines)에 추가로 attention을 적용하여 각 feature별 가중치까지 학습하게 만든 AFM(Attentional Factorization Machines) 소개
    - [논문요약글](https://lih0905.github.io/%EC%B6%94%EC%B2%9C/AFM/)
    <br>

1. [[Deep Autoencoder for Recommender Systems: Parameter Influence Analysis], Hoan et al. (2018)](https://arxiv.org/abs/1901.00415) ([코드](https://github.com/heroddaji/flexEncoder))
    - 날짜 : 2020-01-07
    - 카테고리 : 추천시스템
    - 내용 : DAE(Deep Autoencoder)를 이용하여 각 아이템에 대한 유저들의 평점을 입력 벡터로 하여 MLP 및 dropout 적용 후 자기 자신을 다시 생성하게 하는 오토인코더를 이용하여 해당 아이템을 추천하지 않은 유저들의 평점을 예측하는 추천 시스템 모델. 특히 다양한 hyperparameter를 반영할 수 있도록 실험 환경을 구축하여 최적의 parameter 탐색에 초점을 둠. 
    <br>

1. [[HoAFM: A High-order Attentive Factorization Machine for CTR Prediction], Tao et al. (2019)](http://staff.ustc.edu.cn/~hexn/papers/IPM19-HoAFM.pdf) ([코드](https://github.com/zltao/HoAFM/blob/master/HoAFM_v1.0.py))
    - 날짜 : 2020-01-08
    - 카테고리 : 추천시스템
    - 내용 : 1. 에서 소개한 AFM은 벡터간의 point-wise곱에 attention scalar를 곱하도록 설계된 모델이다. HoAFM은 이를 좀 더 일반화하여 attention scalar 대신 vector를 곱하는, 다시 말해 point-wise곱의 각 component별로 영향력을 다르게 반영하는 모델이다.
    <br>

1. [[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding], Devlin et al. (2018)](https://arxiv.org/abs/1810.04805) ([코드](https://github.com/dhlee347/pytorchic-bert/))
    - 날짜 : 2020-01-13
    - 카테고리 : NLP
    - 내용 : 그 유명한 BERT. Sentence를 토큰 단위로 Bidirectional Transformer layer에 통과시켜 pre-train시킨 후, 각 task에 대응하는 output 형태로 fine-tuning시키는 모델이다. Pre-train 과정에서 input token의 15%를 실제 학습에 사용하는데, 이중 80%는 마스킹, 10%는 랜덤한 다른 token, 10%는 입력 token을 그대로 사용하여 언어의 구조 및 단어 추론 등을 학습하도록 한다.
    <br>

1. [[BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer], Sun et al. (2019)](https://arxiv.org/abs/1904.06690) ([코드](https://github.com/FeiSun/BERT4Rec))
    - 날짜 : 2020-01-14
    - 카테고리 : 추천시스템
    - 내용 : 제목부터 알 수 있듯이, BERT와 유사한 구조로 사용자가 클릭한 순차적 아이템을 일부 masking한 input을 Bidirectional Transformer를 이용하여 훈련시키는 모델이다. 다만 pre-train을 시키는 것은 아니며, 단지 bidirectional transformer를 활용한다는 점이 BERT와 유사하다.
    <br>

1. [[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations], Lan et al. (2019)](https://arxiv.org/abs/1909.11942) ([코드](https://github.com/google-research/ALBERT))
    - 날짜 : 2020-01-23
    - 카테고리 : NLP
    - 내용 : BERT의 경량화 방법에 대해 고민한 모델 중 하나인 ALBERT를 다룬 논문이다. 기존 BERT 모델에 추가로 Factorized embedding parametrization(임베딩/히든 스페이스의 디멘전을 다르게 설정), Cross-layer parameter sharing(모든 레이어의 파라미터를 공유), Sentence-order prediction(NSP 대신 문장의 순서에 대해 학습) 등의 요소를 도입하였다. 이를 통해 파라미터 수/학습 시간은 줄어들었으나 오히려 성능은 향상되는 놀라운 결과를 낳았다.
    <br>

1. [[Session-based Recommendation with Graph Neural Networks], Wu et al. (2019)](https://arxiv.org/abs/1811.00855) ([코드](https://github.com/CRIPAC-DIG/SR-GNN))
    - 날짜 : 2020-01-30
    - 카테고리 : 추천시스템
    - 내용 : 세션을 그래프로 나타낸 후, 그래프의 Connecting Matrix를 이용하여 GRU structure를 통해 아이템의 벡터 표현을 학습한다. 이후 세션의 각 아이템에 대한 Attention value를 계산하여 세션의 벡터 표현을 구한 후, 이를 토대로 세션의 다음 아이템을 예측한다. 즉, Graph + GRU + Attention 기반의 세션 예측 모델이다.
    - [발표자료](https://github.com/lih0905/lih0905.github.io/raw/master/_posts/SR-GNN.pdf)
    