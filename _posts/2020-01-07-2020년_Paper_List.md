---
title: "2020년 읽은 논문들"
date: 2020-01-07 23:15:28 -0400
categories: 잡설
tags:
  - Papers
  - 논문
  - review

use_math: true
#toc: true
---

2020년 들어 읽은 논문 정리.

1. [[Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks], Xiao et al. (2017)](https://www.comp.nus.edu.sg/~xiangnan/papers/ijcai17-afm.pdf) ([코드](https://github.com/hexiangnan/attentional_factorization_machine)) 
    - 날짜 : 2020-01-06
    - 카테고리 : 추천시스템
    - 내용 : 추천 시스템의 feature를 모두 동등하게 평가하여 분해하는 모델 FM(Factorization Machines)에 추가로 attention을 적용하여 각 feature별 가중치까지 학습하게 만든 AFM(Attentional Factorization Machines) 소개
    - [논문요약글](https://lih0905.github.io/%EC%B6%94%EC%B2%9C/AFM/)
    <br>

1. [[Deep Autoencoder for Recommender Systems: Parameter Influence Analysis], Hoan et al. (2018)](https://arxiv.org/abs/1901.00415) ([코드](https://github.com/heroddaji/flexEncoder))
    - 날짜 : 2020-01-07
    - 카테고리 : 추천시스템
    - 내용 : DAE(Deep Autoencoder)를 이용하여 각 아이템에 대한 유저들의 평점을 입력 벡터로 하여 MLP 및 dropout 적용 후 자기 자신을 다시 생성하게 하는 오토인코더를 이용하여 해당 아이템을 추천하지 않은 유저들의 평점을 예측하는 추천 시스템 모델. 특히 다양한 hyperparameter를 반영할 수 있도록 실험 환경을 구축하여 최적의 parameter 탐색에 초점을 둠. 
    <br>

1. [[HoAFM: A High-order Attentive Factorization Machine for CTR Prediction], Tao et al. (2019)](http://staff.ustc.edu.cn/~hexn/papers/IPM19-HoAFM.pdf) ([코드](https://github.com/zltao/HoAFM/blob/master/HoAFM_v1.0.py))
    - 날짜 : 2020-01-08
    - 카테고리 : 추천시스템
    - 내용 : 1. 에서 소개한 AFM은 벡터간의 point-wise곱에 attention scalar를 곱하도록 설계된 모델이다. HoAFM은 이를 좀 더 일반화하여 attention scalar 대신 vector를 곱하는, 다시 말해 point-wise곱의 각 component별로 영향력을 다르게 반영하는 모델이다.
    <br>

1. [[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding], Devlin et al. (2018)](https://arxiv.org/abs/1810.04805) ([코드](https://github.com/dhlee347/pytorchic-bert/))
    - 날짜 : 2020-01-13
    - 카테고리 : NLP
    - 내용 : 그 유명한 BERT. Sentence를 토큰 단위로 Bidirectional Transformer layer에 통과시켜 pre-train시킨 후, 각 task에 대응하는 output 형태로 fine-tuning시키는 모델이다. Pre-train 과정에서 input token의 15%를 실제 학습에 사용하는데, 이중 80%는 마스킹, 10%는 랜덤한 다른 token, 10%는 입력 token을 그대로 사용하여 언어의 구조 및 단어 추론 등을 학습하도록 한다.
    <br>

1. [[BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer], Sun et al. (2019)](https://arxiv.org/abs/1904.06690) ([코드](https://github.com/FeiSun/BERT4Rec))
    - 날짜 : 2020-01-14
    - 카테고리 : 추천시스템
    - 내용 : 제목부터 알 수 있듯이, BERT와 유사한 구조로 사용자가 클릭한 순차적 아이템을 일부 masking한 input을 Bidirectional Transformer를 이용하여 훈련시키는 모델이다. 다만 pre-train을 시키는 것은 아니며, 단지 bidirectional transformer를 활용한다는 점이 BERT와 유사하다.
    <br>

1. [[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations], Lan et al. (2019)](https://arxiv.org/abs/1909.11942) ([코드](https://github.com/google-research/ALBERT))
    - 날짜 : 2020-01-23
    - 카테고리 : NLP
    - 내용 : BERT의 경량화 방법에 대해 고민한 모델 중 하나인 ALBERT를 다룬 논문이다. 기존 BERT 모델에 추가로 Factorized embedding parametrization(임베딩/히든 스페이스의 디멘전을 다르게 설정), Cross-layer parameter sharing(모든 레이어의 파라미터를 공유), Sentence-order prediction(NSP 대신 문장의 순서에 대해 학습) 등의 요소를 도입하였다. 이를 통해 파라미터 수/학습 시간은 줄어들었으나 오히려 성능은 향상되는 놀라운 결과를 낳았다.
    <br>

1. [[Session-based Recommendation with Graph Neural Networks], Wu et al. (2019)](https://arxiv.org/abs/1811.00855) ([코드](https://github.com/CRIPAC-DIG/SR-GNN))
    - 날짜 : 2020-01-30
    - 카테고리 : 추천시스템
    - 내용 : 세션을 그래프로 나타낸 후, 그래프의 Connecting Matrix를 이용하여 GRU structure를 통해 아이템의 벡터 표현을 학습한다. 이후 세션의 각 아이템에 대한 Attention value를 계산하여 세션의 벡터 표현을 구한 후, 이를 토대로 세션의 다음 아이템을 예측한다. 즉, Graph + GRU + Attention 기반의 세션 예측 모델이다.
    - [발표자료](https://github.com/lih0905/lih0905.github.io/raw/master/_posts/SR-GNN.pdf)
    <br>

1. [[Online Deep Learning: Learning Deep Neural Networks on the Fly], Sahoo et al. (2018)](https://www.ijcai.org/Proceedings/2018/0369.pdf)
    - 날짜 : 2020-03-04
    - 카테고리 : Online Learning
    <br>

1. [[Autonomous Deep Learning: Continual Learning Approach for Dynamic Environments], Ashfahani et al (2020)](https://arxiv.org/abs/1810.07348v4)
    - 날짜 : 2020-03-04
    - 카테고리 : Online Learning
    - [코드](https://github.com/SeptivianaSavitri/adl_python)
    <br>

1. [[Online Learning to Rank for Sequential Music Recommendation], Pereira et al.(2019)](https://homepages.dcc.ufmg.br/~rodrygo/wp-content/papercite-data/pdf/pereira2019recsys.pdf)
    - 날짜 : 2020-03-26
    - 카테고리 : 추천시스템, Online Learning
    - 내용 : 실시간 음악 추천 시스템을 구현하는 방법에 대한 논문. 먼저 기존 유저의 선호도를 이용하여 추천 리스트를 만들어둔 후, 유저의 선택(끝까지 듣는지 혹은 중간에 스킵하는지)에 따라 리스트를 실시간으로 학습시켜가는 방법론을 다루고 있다.
    <br>

1. [[Error-Driven Incremental Learning in Deep Convolutional Neural Network for Large-Scale Image Classiﬁcation], Xiao et al. (2014)](https://dl.acm.org/doi/pdf/10.1145/2647868.2654926)
    - 날짜 : 2020-04-02
    - 카테고리 : Incremental Learning
    <br>

1. [[iCaRL: Incremental Classifier and Representation Learning], Rebuffi et al. (2016)](http://openaccess.thecvf.com/content_cvpr_2017/papers/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.pdf)
    - 날짜 : 2020-04-09
    - 카테고리 : Incremental Learning
    - 내용 : 이미지 분류 태스크에서 클래스가 지속적으로 증가하는 경우에 어떤 식으로 모델을 구성해야 하는지를 다룬 논문. 먼저 기존 데이터를 토대로 훈련시킨 CNN모델을 생각한 후, 각 클래스별 대표 이미지(exemplar sets)를 클래스 전체 이미지의 모델 출력의 평균값과 가장 가까운 이미지들로 선택한다. 이제 새로운 데이터가 들어오면, 기존 클래스에 대해서는 distillation loss, 새로운 클래스에 대해서는 classification loss를 적용하여 모델을 훈련시킨다. 또한 모든 클래스에 대해서 지속해서 대표 이미지를 업데이트하여 모델을 유지시켜 나가는 모델이다.
    - [발표자료](https://www.youtube.com/watch?v=HCKi41BHDAk)
    <br>
    
1. [[Improving Language Understanding by Generative Pre-Training], Radford et al. (2019)](https://www.google.com/url?q=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&sa=U&ved=2ahUKEwiVhrzvlePpAhWbBIgKHTPiCGMQFjAJegQIABAB&usg=AOvVaw1UMlQhOKGc0dmX7SbHNNHo)
    - 날짜 : 2020-05-24
    - 카테고리 : NLP
    - 내용 : Unsupervised pre-training으로 트랜스포머 디코더를 이용하고, 이후 supervised fine-tuning을 수행하는 모델. Fine-tuning 과정에서는 pre-traning loss를 도움 함수로 추가해준다.
    <br>

1. [[XLNet: Generalized Autoregressive Pretraining for Language Understanding], Yang et al. (2019)]
    - 날짜 : 2020-05-31
    - 카테고리 : NLP
    - 내용 : 한방향의 정보만 이해하는 GPT, 양방향이지만 [mask] 토큰의 존재로 인해 정보를 유실하는 BERT의 단점을 극복하기 위해 나온 모델.
    <br>